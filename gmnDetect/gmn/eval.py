'''
    Use evaluation dataset generated by gen_eval.py to evaluate saved model
'''

# from evaluation import compute_similarity, auc, roc
from evaluation import *
from loss import pairwise_loss, triplet_loss
from utils import *
from configure import *
import collections
import time
import os
import sys
from graphMatrix.config import GRAPH_TYPES, GRAPH_MODE


from argparse import ArgumentParser

parser = ArgumentParser("train disjoint.")
parser.add_argument("--project", type=str, default="curl")
parser.add_argument("--cve_id", type=str, default="CVE-2021-22901")
parser.add_argument("--gpu", type=str, default="0")
parser.add_argument("--hl", action="store_true", default=False)
args = parser.parse_args()

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu

import numpy as np
import torch.nn as nn

# Set GPU

use_cuda = torch.cuda.is_available()
print("****************", use_cuda, args.gpu)
device = torch.device("cuda:0" if use_cuda else "cpu")

hl = args.hl
project = args.project
cve = args.cve_id

# 如果高亮方案通过扩充维度执行
if GRAPH_MODE == 'single':
    if hl:
        # edge_feature_dim = 2
        edge_feature_dim = 1
    else:
        edge_feature_dim = 1
else:
    if hl:
        edge_feature_dim = len(GRAPH_TYPES) + 1
    else:
        edge_feature_dim = len(GRAPH_TYPES)

if GRAPH_MODE == "disjoint":
    # config = get_disjoint_config(hl, edge_feature_dim, batchsize, lr, project, cve, epoch, GRAPH_MODE)
    # for k, v in config.items():
    #     print("%s= %s" % (k, v))
    config = get_disjoint_config(hl = hl, edge_state_dim = edge_feature_dim, project = project, cve = cve, graphMode = GRAPH_MODE)
    for k, v in config.items():
        print("%s= %s" % (k, v))
else:
    config = get_single_config(hl = hl, edge_state_dim = edge_feature_dim, project = project, cve = cve, graphMode = GRAPH_MODE)
    for k, v in config.items():
        print("%s= %s" % (k, v))

# if len(sys.argv) != 4:
#     print(f"Usage: python eval.py [eval|compare] [project] [cve_id]")
#     exit()

# project = sys.argv[2]
# cve_id = sys.argv[3]
# print("[*] method: {}".format(method))
# print("[*] project: {}".format(project))
# print("[*] cve_id: {}".format(cve_id))


# train_dataset_dir = "../output"
# eval_dataset_dir = "../new_eval_box"
# compare_dataset_dir = "./compare_box"
save_cpkt_dir = "../detector_newpaper/models"

output_path = "output/eval_results"
os.makedirs(output_path, exist_ok=True)

# prefix = "{}/{}".format(project, cve_id)
# config["ckpt_save_path"]
# config['ckpt_save_path'] = '{}/{}.pkl'.format(save_cpkt_dir, prefix)
# config['data']['dataset_params']['training_dataset_dir']  = '{}/{}/train/'.format(train_dataset_dir, prefix)
# config['data']['dataset_params']['validation_dataset_dir']  = '{}/{}/test/'.format(train_dataset_dir, prefix)
# config['data']['dataset_params']['eval_dataset_dir']  = '{}/{}/'.format(eval_dataset_dir, prefix)

# if method == "compare":
#     config['data']['dataset_params']['compare_path']  = '{}/{}/'.format(compare_dataset_dir, prefix)

for (k, v) in config.items():
    print("%s= %s" % (k, v))

# Set random seeds
seed = config['seed']
random.seed(seed)
np.random.seed(seed + 1)
torch.manual_seed(seed + 2)
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.benchmark = True

config['data']['problem'] = 'malicious_detection_test'

# if method == "eval":
#     config['data']['problem'] = 'malicious_detection_test'
# elif method == "compare":
#     config['data']['problem'] = 'malicious_detection_compare'
# else:
#     print("[+] Unknown method: {}".format(method))
#     exit(1)

print("[info] Build datasets")
validation_set = build_datasets(config)

if config['training']['mode'] == 'pair':
    # print("config['training']['batch_size']", config['training']['batch_size'])
    # validation_data_iter = validation_set.pairs(config['training']['batch_size'])
    validation_data_iter = validation_set.pairs()
    first_batch_graphs, _ = next(validation_data_iter)
else:
    validation_data_iter = validation_set.triplets(config['training']['batch_size'])
    first_batch_graphs = next(validation_data_iter)

node_feature_dim = first_batch_graphs.node_features.shape[-1]
print(node_feature_dim)
edge_feature_dim = 1

# model, optimizer = build_model(config, node_feature_dim, edge_feature_dim)
# model.to(device)

###############################################
print("**********config['ckpt_save_path']", config['ckpt_save_path'])
if hl:
    model_path = os.path.join("../../data/", args.project, args.cve_id, "model_hl", GRAPH_MODE) + '/' + args.cve_id + '.pkl'
else:
    model_path = os.path.join("../../data/", args.project, args.cve_id, "model", GRAPH_MODE) + '/' + args.cve_id + '.pkl'

# if os.path.isfile(model_path):
    # checkpoint = torch.load(config['ckpt_save_path'])
    # model.load_state_dict(checkpoint['model_state_dict'])
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    # model = torch.load(config['ckpt_save_path'])
model = torch.load(model_path)
model.to(device)
print('model reloaded from ckpt~')
# else:
#     print('failed')
#     exit(1)
###############################################

accumulated_metrics = collections.defaultdict(list)

metrics_to_print = {
    k: torch.mean(v[0]) for k, v in accumulated_metrics.items() if not k == 'pair_auc(train)'}
if config['training']['mode'] == 'pair':
    metrics_to_print['pair_auc(train)'] = accumulated_metrics['pair_auc(train)']
print(metrics_to_print)

# reset the metrics
accumulated_metrics = collections.defaultdict(list)
max_diff_1 = 0
min_diff_n1 = 9999999999
model.eval()

diff_1 = []
diff_n1 = []
all_scores = np.array([])
all_labels = np.array([])
with torch.no_grad():
    accumulated_pair_auc = []
    batch_idx = 0
    for batch in validation_set.pairs():
        # 一个batchsize的数据直接投进去
    # for batch in validation_set.pairs(batch_size=config['evaluation']['batch_size']):
        batch_idx += 1
        node_features, edge_features, from_idx, to_idx, graph_idx, labels = get_graph(batch)
        labels = labels.to(device)
        eval_pairs = model(node_features.to(device), edge_features.to(device), from_idx.to(device),
                            to_idx.to(device),
                            graph_idx.to(device), config['evaluation']['batch_size'] * 2)

        x, y = reshape_and_split_tensor(eval_pairs, 2)
        similarity = compute_similarity(config, x, y)

        # find biggest diff / smallest diff & save diffs
        for i in range(config['evaluation']['batch_size']):
            if labels[i] == 1:
                # 取最大的相似值一直更新max_diff_1
                max_diff_1 = abs(similarity[i]) if abs(similarity[i]) > max_diff_1 else max_diff_1
                diff_1.append(str(abs(float(similarity[i]))) + '\n')
            else:
                # 取最小的相似值一直更新min_diff_n1
                min_diff_n1 = abs(similarity[i]) if abs(similarity[i]) < min_diff_n1 else min_diff_n1
                diff_n1.append(str(abs(float(similarity[i]))) + '\n')
            # print(abs(float(similarity[i])))
        
        pair_auc = auc(similarity, labels)
        accumulated_pair_auc.append(pair_auc)
        # all_scores记录了所有的相似度，all_scores记录了所有的标签
        all_scores = np.concatenate((all_scores, similarity.cpu().detach().numpy()))
        all_labels = np.concatenate((all_labels, labels.cpu().detach().numpy()))
        print("[info] batch: {}, auc: {}, all_scores: {}, all_labels: {}".format(
            batch_idx, pair_auc, all_scores.shape, all_labels.shape
        ))

        sys.stdout.flush()
        # if batch_idx > 50:
        #     break

# 结束所有轮batch size个数据验证后
    print(max_diff_1, min_diff_n1)

    eval_metrics = {
        'pair_auc(vali)': np.mean(accumulated_pair_auc)}
    print("pair_auc(vali)",eval_metrics)

    f = open("{}/diff_1_{}_{}".format(output_path, project, cve), 'w')
    f.writelines(diff_1)

    f = open("{}/diff_n1_{}_{}".format(output_path, project, cve), 'w')
    f.writelines(diff_n1)

    all_fpr, all_tpr, all_thresholds = roc(all_scores, all_labels)
    print("[info] all_scores: {}, all_labels: {}, all_fpr: {}, all_tpr: {}, all_thresholds: {}".format(
        all_scores.shape, all_labels.shape, all_fpr.shape, all_tpr.shape, all_thresholds.shape
    ))

    with open("{}/all_scores_{}_{}".format(output_path, project, cve_id), "w") as f:
        for x in all_scores.tolist():
            f.write(str(x) + "\n")

    with open("{}/all_labels_{}_{}".format(output_path, project, cve_id), "w") as f:
        for x in all_labels.tolist():
            f.write(str(x) + "\n")

    with open("{}/all_fpr_{}_{}".format(output_path, project, cve_id), "w") as f:
        for x in all_fpr.tolist():
            f.write(str(x) + "\n")

    with open("{}/all_tpr_{}_{}".format(output_path, project, cve_id), "w") as f:
        for x in all_tpr.tolist():
            f.write(str(x) + "\n")

    with open("{}/all_thresholds_{}_{}".format(output_path, project, cve_id), "w") as f:
        for x in all_thresholds.tolist():
            f.write(str(x) + "\n")

